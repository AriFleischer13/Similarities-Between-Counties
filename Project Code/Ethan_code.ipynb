{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_2015_county_data = pd.read_csv(\"data/full_2015_county_data.csv\")\n",
    "full_2015_county_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_2017_county_data = pd.read_csv(\"data/full_2017_county_data.csv\")\n",
    "\n",
    "full_2017_county_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_unsupervised_2015 = full_2015_county_data.iloc[:, 6:]\n",
    "full_unsupervised_2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_unsupervised_2017 = full_2017_county_data.iloc[:, 6:]\n",
    "full_unsupervised_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_unsupervised_2015['Men'] = full_unsupervised_2015['Men']/full_unsupervised_2015['TotalPop']\n",
    "full_unsupervised_2015['Women'] = full_unsupervised_2015['Women']/full_unsupervised_2015['TotalPop']\n",
    "full_unsupervised_2015['Citizen'] = full_unsupervised_2015['Citizen']/full_unsupervised_2015['TotalPop']\n",
    "\n",
    "full_unsupervised_2015 = full_unsupervised_2015.fillna(0)\n",
    "\n",
    "full_unsupervised_2017['Men'] = full_unsupervised_2017['Men']/full_unsupervised_2017['TotalPop']\n",
    "full_unsupervised_2017['Women'] = full_unsupervised_2017['Women']/full_unsupervised_2017['TotalPop']\n",
    "full_unsupervised_2017['VotingAgeCitizen'] = full_unsupervised_2017['VotingAgeCitizen']/full_unsupervised_2017['TotalPop']\n",
    "\n",
    "full_unsupervised_2017 = full_unsupervised_2017.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "\n",
    "full_normalized_2015_dataset = (full_unsupervised_2015-full_unsupervised_2015.min())/(full_unsupervised_2015.max()-full_unsupervised_2015.min())\n",
    "full_normalized_2015_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_normalized_2015_dataset.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_normalized_2015_dataset.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_normalized_2017_dataset = (full_unsupervised_2017-full_unsupervised_2017.min())/(full_unsupervised_2017.max()-full_unsupervised_2017.min())\n",
    "full_normalized_2017_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_normalized_2017_dataset.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_normalized_2017_dataset.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_county_identifier_2015 = full_2015_county_data.iloc[:, :6]\n",
    "full_county_identifier_2015 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_county_identifier_2017 = full_2017_county_data.iloc[:, :6]\n",
    "full_county_identifier_2017 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continental_2015_county_data = pd.read_csv(\"data/continental_2015_county_data.csv\")\n",
    "continental_2015_county_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continental_2017_county_data = pd.read_csv(\"data/continental_2017_county_data.csv\")\n",
    "continental_2017_county_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continental_unsupervised_2015 = continental_2015_county_data.iloc[:, 6:]\n",
    "continental_unsupervised_2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continental_unsupervised_2017 = continental_2017_county_data.iloc[:, 6:]\n",
    "continental_unsupervised_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continental_normalized_2015_dataset = (continental_unsupervised_2015-continental_unsupervised_2015.min())/(continental_unsupervised_2015.max()-continental_unsupervised_2015.min())\n",
    "continental_normalized_2015_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continental_normalized_2015_dataset.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continental_normalized_2015_dataset.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continental_normalized_2017_dataset = (continental_unsupervised_2017-continental_unsupervised_2017.min())/(continental_unsupervised_2017.max()-continental_unsupervised_2017.min())\n",
    "continental_normalized_2017_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continental_normalized_2017_dataset.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continental_normalized_2017_dataset.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continental_county_identifier_2015 = continental_2015_county_data.iloc[:, :6]\n",
    "continental_county_identifier_2015 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continental_county_identifier_2017 = continental_2017_county_data.iloc[:, :6]\n",
    "continental_county_identifier_2017 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_normalized_2017_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_normalized_2017_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(full_normalized_2017_dataset)):\n",
    "    if full_normalized_2017_dataset['ChildPoverty'].isnull()[i] == True:\n",
    "        print(i)\n",
    "full_normalized_2017_dataset = full_normalized_2017_dataset.drop(labels=[548], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge and Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge\n",
    "n_alphas = 200\n",
    "alpha_vals = np.logspace(-5, 3, n_alphas)\n",
    "\n",
    "df_top_feats = pd.DataFrame()\n",
    "\n",
    "coefs_r = {}\n",
    "max_feats_r = {}\n",
    "for c in range(len(full_normalized_2017_dataset[full_normalized_2017_dataset.columns[:-2]].columns)):\n",
    "    coefs_r[c] = []\n",
    "    max_feats_r[c] = []\n",
    "    for alpha in alpha_vals:\n",
    "        ridge_model = linear_model.Ridge(alpha=alpha)\n",
    "        ridge_model.fit(full_normalized_2017_dataset[full_normalized_2017_dataset.columns[:-2]].values, full_normalized_2017_dataset[full_normalized_2017_dataset.columns[:-2]][full_normalized_2017_dataset[full_normalized_2017_dataset.columns[:-2]].columns[c]].values)\n",
    "        coefs_r[c].append(ridge_model.coef_)\n",
    "\n",
    "    max_list = sorted(((value, index) for index, value in enumerate(coefs_r[c][0])), reverse=True)\n",
    "    for i in range(10):\n",
    "        max_feats_r[c].append(full_normalized_2017_dataset[full_normalized_2017_dataset.columns[:-2]].columns[max_list[i][1]])\n",
    "    df_top_feats[full_normalized_2017_dataset[full_normalized_2017_dataset.columns[:-2]].columns[c]] = max_feats_r[c]\n",
    "        \n",
    "        \n",
    "  # TODO: \n",
    "  # - label top features for each plot\n",
    "  # normalize weights\n",
    "  # plot as fn of ||beta_alpha^R||_2 / ||beta||_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many times each feature appears in top 5 \n",
    "# normalize - each feature is likely a top feature for its own predictor\n",
    "feats = []\n",
    "for c in range(len(full_normalized_2017_dataset[full_normalized_2017_dataset.columns[:-2]].columns)):\n",
    "    for i in range(df_top_feats.shape[0]):\n",
    "        feats.append(df_top_feats[[full_normalized_2017_dataset[full_normalized_2017_dataset.columns[:-2]].columns[c]]].iloc[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(feats).value_counts().plot('bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(len(full_normalized_2017_dataset.columns)):\n",
    "    ax = plt.gca()\n",
    "\n",
    "    ax.plot(alpha_vals, coefs_r[c])\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "    plt.xlabel(\"alpha\")\n",
    "    plt.ylabel(\"weights\")\n",
    "    plt.title(f\"Ridge coefficients as a function of the regularization ({full_normalized_2017_dataset.columns[c]})\")\n",
    "    plt.axis(\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    print('Top Features:')\n",
    "    print(max_feats_r[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lasso\n",
    "n_alphas = 200\n",
    "alpha_vals = np.logspace(-5, 3, n_alphas)\n",
    "\n",
    "coefs_l = {}\n",
    "max_feats_l = {}\n",
    "for c in range(len(full_normalized_2017_dataset.columns)):\n",
    "    coefs_l[c] = []\n",
    "    max_feats_l[c] = []\n",
    "    for alpha in alpha_vals:\n",
    "        lasso_model = linear_model.Lasso(alpha=alpha)\n",
    "        lasso_model.fit(full_normalized_2017_dataset.values, full_normalized_2017_dataset[full_normalized_2017_dataset.columns[c]].values)\n",
    "        coefs_l[c].append(lasso_model.coef_)\n",
    "\n",
    "    max_list = sorted(((value, index) for index, value in enumerate(coefs_l[c][0])), reverse=True)\n",
    "    for i in range(5):\n",
    "        max_feats_l[c].append(full_normalized_2017_dataset.columns[max_list[i][1]])\n",
    "        \n",
    "        \n",
    "  # TODO: \n",
    "  # - label top features for each plot\n",
    "  # normalize weights\n",
    "  # plot as fn of ||beta_alpha^R||_2 / ||beta||_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(len(full_normalized_2017_dataset.columns)):\n",
    "    ax = plt.gca()\n",
    "\n",
    "    ax.plot(alpha_vals, coefs_l[c])\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "    plt.xlabel(\"alpha\")\n",
    "    plt.ylabel(\"weights\")\n",
    "    plt.title(f\"Lasso coefficients as a function of the regularization ({full_normalized_2017_dataset.columns[c]})\")\n",
    "    plt.axis(\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    print('Top Features:')\n",
    "    print(max_feats_l[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from matplotlib import gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "def apply_pca(image_df, seed):\n",
    "#     np.random.seed(seed)\n",
    "#     rndperm = np.random.permutation(image_df.shape[0])\n",
    "#     df_subset = image_df.loc[rndperm[:sample_size],:].copy()\n",
    "    df_subset = image_df.copy()\n",
    "    \n",
    "    n_components = df_subset.shape[1]\n",
    "    pca1 = PCA(n_components=n_components)\n",
    "    pca1.fit(df_subset.values)\n",
    "\n",
    "    # Scree plot / Pareto plot\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    gs = gridspec.GridSpec(3, 2)\n",
    "\n",
    "    ax1 = plt.subplot(gs[0,0])\n",
    "    ax1.scatter([i for i in range(1,n_components+1)], pca1.singular_values_)\n",
    "\n",
    "    ax2 = plt.subplot(gs[0,1])\n",
    "    ax2.bar([i for i in range(1,n_components+1)], pca1.explained_variance_ratio_ *100)\n",
    "    ax2.plot([i for i in range(1,n_components+1)], np.cumsum(pca1.explained_variance_ratio_ *100), color='red')\n",
    "\n",
    "    ax1.set_title('Scree Plot')\n",
    "    ax2.set_title('Pareto Plot')\n",
    "\n",
    "    ax1.set_ylabel('Eigenvalue')\n",
    "    ax2.set_ylabel('Variance Explained')\n",
    "\n",
    "    ax1.set_xlabel('Principal Component')\n",
    "    ax2.set_xlabel('Principal Component')\n",
    "\n",
    "    n_components = 3\n",
    "    pca2 = PCA(n_components=n_components)\n",
    "    x_new = pca2.fit_transform(df_subset.values)\n",
    "\n",
    "    ax3 = plt.subplot(gs[1:3,0:2], projection='3d')\n",
    "    ax3.scatter([i[1] for i in x_new], [i[2] for i in x_new], [i[0] for i in x_new])\n",
    "\n",
    "    ax3.set_title('PCA (D=3)')\n",
    "    ax3.set_xlabel('Principal Component 2')\n",
    "    ax3.set_ylabel('Principal Component 3')\n",
    "    ax3.set_zlabel('Principal Component 1')\n",
    "    \n",
    "#     print('Cumulative Variance: ', np.cumsum(pca2.explained_variance_ratio_ *100))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pca1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from matplotlib import gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "def apply_pca2(image_df, seed):\n",
    "#     np.random.seed(seed)\n",
    "#     rndperm = np.random.permutation(image_df.shape[0])\n",
    "#     df_subset = image_df.loc[rndperm[:sample_size],:].copy()\n",
    "    df_subset = image_df.copy()\n",
    "    \n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    gs = gridspec.GridSpec(1, 1)\n",
    "\n",
    "    n_components = 3\n",
    "    pca2 = KernelPCA(kernel=\"sigmoid\", fit_inverse_transform=True, n_components=n_components)\n",
    "    x_new = pca2.fit_transform(df_subset.values)\n",
    "\n",
    "    ax3 = plt.subplot(gs[0,0], projection='3d')\n",
    "    ax3.scatter([i[1] for i in x_new], [i[2] for i in x_new], [i[0] for i in x_new])\n",
    "\n",
    "    ax3.set_title('PCA (D=3)')\n",
    "    ax3.set_xlabel('Principal Component 2')\n",
    "    ax3.set_ylabel('Principal Component 3')\n",
    "    ax3.set_zlabel('Principal Component 1')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca1 = apply_pca(full_normalized_2017_dataset[full_normalized_2017_dataset.columns[:-2]], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_pca2(full_normalized_2017_dataset[full_normalized_2017_dataset.columns[:-2]], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    f = plt.figure(figsize=(20, 12))\n",
    "    gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "    ax1 = plt.subplot(gs[0,0])\n",
    "    ax1.bar(full_normalized_2017_dataset.columns[:-2], np.abs(pca1.components_[i]))\n",
    "    ax1.set_title(f'PC{i}')\n",
    "    plt.xticks(rotation=75)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_var = pd.DataFrame(columns = full_normalized_2017_dataset.columns[:-2])\n",
    "df_pca_var = df_pca_var.T\n",
    "\n",
    "for i in range(len(pca1.components_)):\n",
    "    df_pca_var[i] = pca1.components_[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means\n",
    "# K-Means\n",
    "def apply_kmeans(image_df, seed, x, y):\n",
    "    df_subset = image_df.copy()\n",
    "    \n",
    "    image_kmeans = []\n",
    "\n",
    "    for k in [2,3,4,5,6,7,8,9]:\n",
    "        image_kmeans.append(KMeans(n_clusters=k, random_state=0).fit(df_subset.values))\n",
    "        \n",
    "    colors = {}\n",
    "    colors[0] = image_kmeans[0].predict(df_subset.values)\n",
    "    colors[1] = image_kmeans[1].predict(df_subset.values)\n",
    "    colors[2] = image_kmeans[2].predict(df_subset.values)\n",
    "    colors[3] = image_kmeans[3].predict(df_subset.values)\n",
    "    colors[4] = image_kmeans[4].predict(df_subset.values)\n",
    "    colors[5] = image_kmeans[5].predict(df_subset.values)\n",
    "    colors[6] = image_kmeans[6].predict(df_subset.values)\n",
    "    colors[7] = image_kmeans[7].predict(df_subset.values)\n",
    "\n",
    "    f = plt.figure(figsize=(15, 30))\n",
    "    gs = gridspec.GridSpec(4, 2)\n",
    "\n",
    "    ax1 = plt.subplot(gs[0,0])\n",
    "    ax1.scatter(x, y, c=colors[0], s=2)\n",
    "\n",
    "    ax2 = plt.subplot(gs[0,1])\n",
    "    ax2.scatter(x, y, c=colors[1], s=2)\n",
    "\n",
    "    ax3 = plt.subplot(gs[1,0])\n",
    "    ax3.scatter(x, y, c=colors[2], s=2)\n",
    "\n",
    "    ax4 = plt.subplot(gs[1,1])\n",
    "    ax4.scatter(x, y, c=colors[3], s=2)\n",
    "    \n",
    "    ax5 = plt.subplot(gs[2,0])\n",
    "    ax5.scatter(x, y, c=colors[4], s=2)\n",
    "\n",
    "    ax6 = plt.subplot(gs[2,1])\n",
    "    ax6.scatter(x, y, c=colors[5], s=2)\n",
    "    \n",
    "    ax7 = plt.subplot(gs[3,0])\n",
    "    ax7.scatter(x, y, c=colors[6], s=2)\n",
    "\n",
    "    ax8 = plt.subplot(gs[3,1])\n",
    "    ax8.scatter(x, y, c=colors[7], s=2)\n",
    "\n",
    "    ax1.set_title('k=2')\n",
    "    ax2.set_title('k=3')\n",
    "    ax3.set_title('k=4')\n",
    "    ax4.set_title('k=5')\n",
    "    ax5.set_title('k=6')\n",
    "    ax6.set_title('k=7')\n",
    "    ax7.set_title('k=8')\n",
    "    ax8.set_title('k=9')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    k_vals = [2,3,4,5,6,7,8,9]\n",
    "    silhouette_values = []\n",
    "    for i,k in enumerate(k_vals):\n",
    "        sil = silhouette_score(df_subset.values, image_kmeans[i].predict(df_subset.values))\n",
    "        ch = calinski_harabasz_score(df_subset.values, image_kmeans[i].predict(df_subset.values))\n",
    "\n",
    "        print(f'k={k}: Silhouette score = {sil}, CH index = {ch}')\n",
    "        \n",
    "    return colors\n",
    "\n",
    "# DBSCAN\n",
    "# Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HC\n",
    "def apply_hc(image_df, seed, linkage,x,y):\n",
    "    df_subset = image_df.copy()\n",
    "    \n",
    "    image_hc = []\n",
    "\n",
    "    for k in [2,3,4,5,6,7,8,9]:\n",
    "        image_hc.append(AgglomerativeClustering(n_clusters=k, linkage=linkage, affinity='euclidean').fit(df_subset.values))\n",
    "        \n",
    "    colors = {}\n",
    "    colors[0] = image_hc[0].labels_\n",
    "    colors[1] = image_hc[1].labels_\n",
    "    colors[2] = image_hc[2].labels_\n",
    "    colors[3] = image_hc[3].labels_\n",
    "    colors[4] = image_hc[4].labels_\n",
    "    colors[5] = image_hc[5].labels_\n",
    "    colors[6] = image_hc[6].labels_\n",
    "    colors[7] = image_hc[7].labels_\n",
    "    \n",
    "    models = {}\n",
    "    models[0] = AgglomerativeClustering(n_clusters=2, linkage=linkage, affinity='euclidean').get_params(deep=True)\n",
    "    models[1] = AgglomerativeClustering(n_clusters=3, linkage=linkage, affinity='euclidean').get_params(deep=True)\n",
    "    models[2] = AgglomerativeClustering(n_clusters=4, linkage=linkage, affinity='euclidean').get_params(deep=True)\n",
    "    models[3] = AgglomerativeClustering(n_clusters=5, linkage=linkage, affinity='euclidean').get_params(deep=True)\n",
    "    models[4] = AgglomerativeClustering(n_clusters=6, linkage=linkage, affinity='euclidean').get_params(deep=True)\n",
    "    models[5] = AgglomerativeClustering(n_clusters=7, linkage=linkage, affinity='euclidean').get_params(deep=True)\n",
    "    models[6] = AgglomerativeClustering(n_clusters=8, linkage=linkage, affinity='euclidean').get_params(deep=True)\n",
    "    models[7] = AgglomerativeClustering(n_clusters=9, linkage=linkage, affinity='euclidean').get_params(deep=True)\n",
    "\n",
    "    f = plt.figure(figsize=(15, 20))\n",
    "    gs = gridspec.GridSpec(4, 2)\n",
    "\n",
    "    ax1 = plt.subplot(gs[0,0])\n",
    "    ax1.scatter(x, y, c=colors[0], s=2)\n",
    "\n",
    "    ax2 = plt.subplot(gs[0,1])\n",
    "    ax2.scatter(x, y, c=colors[1], s=2)\n",
    "\n",
    "    ax3 = plt.subplot(gs[1,0])\n",
    "    ax3.scatter(x, y, c=colors[2], s=2)\n",
    "\n",
    "    ax4 = plt.subplot(gs[1,1])\n",
    "    ax4.scatter(x, y, c=colors[3], s=2)\n",
    "    \n",
    "    ax5 = plt.subplot(gs[2,0])\n",
    "    ax5.scatter(x, y, c=colors[4], s=2)\n",
    "\n",
    "    ax6 = plt.subplot(gs[2,1])\n",
    "    ax6.scatter(x, y, c=colors[5], s=2)\n",
    "    \n",
    "    ax7 = plt.subplot(gs[3,0])\n",
    "    ax7.scatter(x, y, c=colors[6], s=2)\n",
    "\n",
    "    ax8 = plt.subplot(gs[3,1])\n",
    "    ax8.scatter(x, y, c=colors[7], s=2)\n",
    "\n",
    "    ax1.set_title('k=2')\n",
    "    ax2.set_title('k=3')\n",
    "    ax3.set_title('k=4')\n",
    "    ax4.set_title('k=5')\n",
    "    ax5.set_title('k=6')\n",
    "    ax6.set_title('k=7')\n",
    "    ax7.set_title('k=8')\n",
    "    ax8.set_title('k=9')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    k_vals = [2,3,4,5,6,7,8,9]\n",
    "    silhouette_values = []\n",
    "    for i,k in enumerate(k_vals):\n",
    "        sil = silhouette_score(df_subset.values, image_hc[i].labels_)\n",
    "        ch = calinski_harabasz_score(df_subset.values, image_hc[i].labels_)\n",
    "\n",
    "        print(f'k={k}: Silhouette score = {sil}, CH index = {ch}')\n",
    "        \n",
    "    return models, colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM\n",
    "def apply_gmm(image_df, seed, x, y):\n",
    "    df_subset = image_df.copy()\n",
    "    \n",
    "    image_gmm = []\n",
    "\n",
    "    for k in [2,3,4,5,6,7]:\n",
    "        image_gmm.append(GaussianMixture(n_components=k, random_state=0).fit(df_subset.values))\n",
    "        \n",
    "    colors = {}\n",
    "    colors[0] = image_gmm[0].predict(df_subset.values)\n",
    "    colors[1] = image_gmm[1].predict(df_subset.values)\n",
    "    colors[2] = image_gmm[2].predict(df_subset.values)\n",
    "    colors[3] = image_gmm[3].predict(df_subset.values)\n",
    "    colors[4] = image_gmm[4].predict(df_subset.values)\n",
    "    colors[5] = image_gmm[5].predict(df_subset.values)\n",
    "\n",
    "    f = plt.figure(figsize=(15, 20))\n",
    "    gs = gridspec.GridSpec(3, 2)\n",
    "\n",
    "    ax1 = plt.subplot(gs[0,0])\n",
    "    ax1.scatter(x, y, c=colors[0], s=2)\n",
    "\n",
    "    ax2 = plt.subplot(gs[0,1])\n",
    "    ax2.scatter(x, y, c=colors[1], s=2)\n",
    "\n",
    "    ax3 = plt.subplot(gs[1,0])\n",
    "    ax3.scatter(x, y, c=colors[2], s=2)\n",
    "\n",
    "    ax4 = plt.subplot(gs[1,1])\n",
    "    ax4.scatter(x, y, c=colors[3], s=2)\n",
    "    \n",
    "    ax5 = plt.subplot(gs[2,0])\n",
    "    ax5.scatter(x, y, c=colors[4], s=2)\n",
    "\n",
    "    ax6 = plt.subplot(gs[2,1])\n",
    "    ax6.scatter(x, y, c=colors[5], s=2)\n",
    "\n",
    "    ax1.set_title('k=2')\n",
    "    ax2.set_title('k=3')\n",
    "    ax3.set_title('k=4')\n",
    "    ax4.set_title('k=5')\n",
    "    ax5.set_title('k=6')\n",
    "    ax6.set_title('k=7')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    k_vals = [2,3,4,5,6,7]\n",
    "    silhouette_values = []\n",
    "    for i,k in enumerate(k_vals):\n",
    "        sil = silhouette_score(df_subset.values, image_gmm[i].predict(df_subset.values))\n",
    "        ch = calinski_harabasz_score(df_subset.values, image_gmm[i].predict(df_subset.values))\n",
    "\n",
    "        print(f'k={k}: Silhouette score = {sil}, CH index = {ch}')\n",
    "        \n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_pca_tsne(image_df, seed, colors, perp_values):\n",
    "    df_subset = image_df.copy()\n",
    "    \n",
    "    f = plt.figure(figsize=(16, 12))\n",
    "    gs = gridspec.GridSpec(2, 6)\n",
    "    \n",
    "    n_components = 3\n",
    "    pca = PCA(n_components=n_components)\n",
    "    x_new = pca.fit_transform(df_subset.values)\n",
    "\n",
    "    ax1 = plt.subplot(gs[0,1:4], projection='3d')\n",
    "    ax1.scatter([i[1] for i in x_new], [i[2] for i in x_new], [i[0] for i in x_new], c=colors, s=4)\n",
    "\n",
    "    ax1.set_title('PCA (D=3)')\n",
    "    ax1.set_xlabel('Principal Component 2')\n",
    "    ax1.set_ylabel('Principal Component 3')\n",
    "    ax1.set_zlabel('Principal Component 1')\n",
    "    \n",
    "    X_tsne1 = TSNE(n_components=2, perplexity=perp_values[0], learning_rate='auto', init='pca').fit_transform(df_subset.values)\n",
    "    X_tsne2 = TSNE(n_components=2, perplexity=perp_values[1], learning_rate='auto', init='pca').fit_transform(df_subset.values)\n",
    "    X_tsne3 = TSNE(n_components=2, perplexity=perp_values[2], learning_rate='auto', init='pca').fit_transform(df_subset.values)\n",
    "    \n",
    "    ax2 = plt.subplot(gs[1,0:2])\n",
    "    ax2.scatter([i[0] for i in X_tsne1], [i[1] for i in X_tsne1], c=colors, s=4)\n",
    "    \n",
    "    ax3 = plt.subplot(gs[1,2:4])\n",
    "    ax3.scatter([i[0] for i in X_tsne2], [i[1] for i in X_tsne2], c=colors, s=4)\n",
    "    \n",
    "    ax4 = plt.subplot(gs[1,4:6])\n",
    "    ax4.scatter([i[0] for i in X_tsne3], [i[1] for i in X_tsne3], c=colors, s=4)\n",
    "    \n",
    "    ax2.set_title(f'TSNE (perp={perp_values[0]})')\n",
    "    ax3.set_title(f'TSNE (perp={perp_values[1]})')\n",
    "    ax4.set_title(f'TSNE (perp={perp_values[2]})')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_kmeans = {}\n",
    "colors_kmeans[0] = apply_kmeans(full_normalized_2017_dataset, seed=0, x=df_kmeans['Long'], y=df_kmeans['Lat'])\n",
    "# k=4 seems to be a reasonable estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_pca_tsne(full_normalized_2017_dataset, seed=0, colors=colors_kmeans[0][5], perp_values = [5, 30, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate what each cluster is\n",
    "df_kmeans = full_normalized_2017_dataset.copy()\n",
    "df_kmeans['cluster'] = colors_kmeans[0][2]\n",
    "\n",
    "df_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(20, 12))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "ax1 = plt.subplot(gs[0,0])\n",
    "ax1.scatter(df_kmeans['Long'], df_kmeans['Lat'], c=df_kmeans['cluster'], s=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try again with unlabeled data\n",
    "colors_kmeans = {}\n",
    "colors_kmeans[0] = apply_kmeans(full_normalized_2017_dataset[full_normalized_2017_dataset.columns[:-2]], seed=0, x=df_kmeans['Long'], y=df_kmeans['Lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_pca_tsne(full_normalized_2017_dataset[full_normalized_2017_dataset.columns[:-2]], seed=0, colors=colors_kmeans[0][5], perp_values = [5, 30, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate what each cluster is\n",
    "df_kmeans = full_normalized_2017_dataset.copy()\n",
    "df_kmeans['cluster'] = colors_kmeans[0][5]\n",
    "\n",
    "df_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne1 = TSNE(n_components=2, perplexity=30, learning_rate='auto', init='pca').fit_transform(full_normalized_2017_dataset[full_normalized_2017_dataset.columns[:-2]].values)\n",
    "\n",
    "colors = ['blue', 'green', 'red', 'yellow', 'orange', 'purple', 'cyan']\n",
    "\n",
    "f = plt.figure(figsize=(20, 8))\n",
    "gs = gridspec.GridSpec(1,3)\n",
    "\n",
    "ax1 = plt.subplot(gs[0,0:2])\n",
    "sc = ax1.scatter(df_kmeans['Long'], df_kmeans['Lat'], c=[colors[i] for i in df_kmeans['cluster']], s=4)\n",
    "ax1.legend(handles=sc.legend_elements()[0], labels=[0,1,2,3,4,5])\n",
    "\n",
    "ax1.set_xlabel('Normalized Long')\n",
    "ax1.set_ylabel('Normalized Lat')\n",
    "\n",
    "ax2 = plt.subplot(gs[0,2])\n",
    "ax2.scatter([i[0] for i in X_tsne1], [i[1] for i in X_tsne1], c=[colors[i] for i in df_kmeans['cluster']], s=4)\n",
    "\n",
    "ax1.set_title('K-Means Clustering')\n",
    "ax2.set_title('K-Means Clustering (t-SNE Visualization)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans['cluster'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = {}\n",
    "\n",
    "for c in list(set(df_kmeans['cluster'].unique())):\n",
    "    df_temp = df_kmeans[df_kmeans['cluster']==c].copy()\n",
    "    means[c] = df_temp.mean().values\n",
    "    \n",
    "df_means = pd.DataFrame()\n",
    "df_means['cl0'] = means[0]\n",
    "df_means['cl1'] = means[1]\n",
    "df_means['cl2'] = means[2]\n",
    "df_means['cl3'] = means[3]\n",
    "df_means['cl4'] = means[4]\n",
    "df_means['cl5'] = means[5]\n",
    "\n",
    "df_means = df_means.T\n",
    "df_means.columns = df_temp.columns\n",
    "df_means = df_means.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_normalized_2017_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors_hc = {}\n",
    "model_hc = {}\n",
    "model_hc[0], colors_hc[0] = apply_hc(full_normalized_2017_dataset[full_normalized_2017_dataset.columns[:-2]], seed=0, linkage='ward', x=full_normalized_2017_dataset['Long'], y=full_normalized_2017_dataset['Lat'])\n",
    "model_hc[1], colors_hc[1] = apply_hc(full_normalized_2017_dataset_no_pr[full_normalized_2017_dataset_no_pr.columns[:-2]], seed=0, linkage='ward', x=full_normalized_2017_dataset_no_pr['Long'], y=full_normalized_2017_dataset_no_pr['Lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as shc\n",
    "Dendrogram = shc.dendrogram((shc.linkage(full_normalized_2017_dataset[full_normalized_2017_dataset.columns[:-2]], method ='ward')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dendrogram = shc.dendrogram((shc.linkage(full_normalized_2017_dataset_no_pr[full_normalized_2017_dataset_no_pr.columns[:-2]], method ='ward')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_pca_tsne(full_normalized_2017_dataset, seed=0, colors=colors_hc[0][6], perp_values = [5, 30, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "df_hc = full_normalized_2017_dataset.copy()\n",
    "df_hc['cluster'] = colors_hc[0][5]\n",
    "\n",
    "X_tsne1 = TSNE(n_components=2, perplexity=30, learning_rate='auto', init='pca').fit_transform(full_normalized_2017_dataset[full_normalized_2017_dataset.columns[:-2]].values)\n",
    "\n",
    "# colors = ['blue', 'cyan', 'purple', 'yellow', 'red', 'green']\n",
    "colors = ['blue', 'red', 'purple', 'yellow', 'cyan', 'green', 'orange']\n",
    "# colors = ['red', 'blue', 'purple', 'yellow', 'cyan', 'green', 'orange', 'brown', 'gray']\n",
    "\n",
    "f = plt.figure(figsize=(20, 8))\n",
    "gs = gridspec.GridSpec(1,3)\n",
    "\n",
    "ax1 = plt.subplot(gs[0,0:2])\n",
    "# sc = ax1.scatter(df_hc['Long'], df_hc['Lat'], c=[colors[i] for i in df_hc['cluster']], label=df_hc['cluster'], s=3)\n",
    "sc = ax1.scatter(df_hc['Long'], df_hc['Lat'], c=df_hc['cluster'], cmap=mcolors.ListedColormap(colors), label=df_hc['cluster'], s=2)\n",
    "ax1.legend(*sc.legend_elements(num=7))\n",
    "# ax1.legend(handles=colors, labels=[0,1,2,3,4,5,6])\n",
    "# legend1 = ax1.legend(*sc.legend_elements(num=7),\n",
    "#                     loc=\"upper right\", title=\"Clusters\")\n",
    "# ax1.add_artist(legend1)\n",
    "\n",
    "ax1.set_xlabel('Normalized Long')\n",
    "ax1.set_ylabel('Normalized Lat')\n",
    "\n",
    "ax2 = plt.subplot(gs[0,2])\n",
    "ax2.scatter([i[0] for i in X_tsne1], [i[1] for i in X_tsne1], c=[colors[i] for i in df_hc['cluster']], s=4)\n",
    "\n",
    "ax1.set_title('Hierchical Clustering')\n",
    "ax2.set_title('Hierchical Clustering (t-SNE Visualization)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe important features in clusters\n",
    "for i in range(7):\n",
    "    df_subset = df_hc[df_hc['cluster'] == i]\n",
    "    means = []\n",
    "    signs = []\n",
    "    for col in df_subset.columns:\n",
    "        if col == 'cluster':\n",
    "            means.append(-1)\n",
    "            signs.append(-2)\n",
    "        else:\n",
    "            means.append(np.abs((df_subset[col].mean() - df_hc[col].mean())))\n",
    "            signs.append(np.sign((df_subset[col].mean() - df_hc[col].mean())))\n",
    "   \n",
    "    df_temp = pd.DataFrame(means).T\n",
    "    df_temp.columns = df_subset.columns\n",
    "    df_subset = df_subset.append(df_temp)\n",
    "    \n",
    "    df_temp = pd.DataFrame(signs).T\n",
    "    df_temp.columns = df_subset.columns\n",
    "    df_subset = df_subset.append(df_temp)\n",
    "    print(f'Cluster {i}:')\n",
    "    print(df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax(), df_subset[df_subset['cluster'] == -2][df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()][0])\n",
    "    df_subset = df_subset.drop(columns=[df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()])\n",
    "    print(df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax(), df_subset[df_subset['cluster'] == -2][df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()][0])\n",
    "    df_subset = df_subset.drop(columns=[df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()])\n",
    "    print(df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax(), df_subset[df_subset['cluster'] == -2][df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()][0])\n",
    "    df_subset = df_subset.drop(columns=[df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()])\n",
    "    print(df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax(), df_subset[df_subset['cluster'] == -2][df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()][0])\n",
    "    df_subset = df_subset.drop(columns=[df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()])\n",
    "    print(df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax(), df_subset[df_subset['cluster'] == -2][df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()][0])\n",
    "    df_subset = df_subset.drop(columns=[df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()])\n",
    "    print(df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax(), df_subset[df_subset['cluster'] == -2][df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()][0])\n",
    "    df_subset = df_subset.drop(columns=[df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()])\n",
    "    print(df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax(), df_subset[df_subset['cluster'] == -2][df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()][0])\n",
    "    df_subset = df_subset.drop(columns=[df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()])\n",
    "    print(df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax(), df_subset[df_subset['cluster'] == -2][df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()][0])\n",
    "    df_subset = df_subset.drop(columns=[df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()])\n",
    "    print(df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax(), df_subset[df_subset['cluster'] == -2][df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()][0])\n",
    "    df_subset = df_subset.drop(columns=[df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()])\n",
    "    print(df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax(), df_subset[df_subset['cluster'] == -2][df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()][0])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cn = df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()\n",
    "# df_subset[df_subset['cluster'] == -2][df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hc = full_normalized_2017_dataset_no_pr.copy()\n",
    "df_hc['cluster'] = colors_hc[1][7]\n",
    "\n",
    "X_tsne1 = TSNE(n_components=2, perplexity=30, learning_rate='auto', init='pca').fit_transform(full_normalized_2017_dataset_no_pr[full_normalized_2017_dataset_no_pr.columns[:-2]].values)\n",
    "\n",
    "# colors = ['blue', 'cyan', 'purple', 'yellow', 'red', 'green']\n",
    "# colors = ['blue', 'purple', 'red', 'yellow', 'cyan', 'green']\n",
    "colors = ['red', 'blue', 'purple', 'yellow', 'cyan', 'brown', 'orange', 'gray', 'green']\n",
    "\n",
    "f = plt.figure(figsize=(20, 8))\n",
    "gs = gridspec.GridSpec(1,3)\n",
    "\n",
    "ax1 = plt.subplot(gs[0,0:2])\n",
    "sc = ax1.scatter(df_hc['Long'], df_hc['Lat'], c=[colors[i] for i in df_hc['cluster']], s=6)\n",
    "sc = ax1.scatter(df_hc['Long'], df_hc['Lat'], c=df_hc['cluster'], cmap=mcolors.ListedColormap(colors), label=df_hc['cluster'], s=2)\n",
    "ax1.legend(*sc.legend_elements(num=9))\n",
    "\n",
    "ax1.set_xlabel('Normalized Long')\n",
    "ax1.set_ylabel('Normalized Lat')\n",
    "\n",
    "ax2 = plt.subplot(gs[0,2])\n",
    "ax2.scatter([i[0] for i in X_tsne1], [i[1] for i in X_tsne1], c=[colors[i] for i in df_hc['cluster']], s=4)\n",
    "\n",
    "ax1.set_title('Hierchical Clustering')\n",
    "ax2.set_title('Hierarchical Clustering (t-SNE Visualization)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe important features in clusters\n",
    "for i in range(len(df_hc['cluster'].unique())):\n",
    "    df_subset = df_hc[df_hc['cluster'] == i]\n",
    "    means = []\n",
    "    signs = []\n",
    "    for col in df_subset.columns:\n",
    "        if col == 'cluster':\n",
    "            means.append(-1)\n",
    "            signs.append(-2)\n",
    "        else:\n",
    "            means.append(np.abs((df_subset[col].mean() - df_hc[col].mean())))\n",
    "            signs.append(np.sign((df_subset[col].mean() - df_hc[col].mean())))\n",
    "   \n",
    "    df_temp = pd.DataFrame(means).T\n",
    "    df_temp.columns = df_subset.columns\n",
    "    df_subset = df_subset.append(df_temp)\n",
    "    \n",
    "    df_temp = pd.DataFrame(signs).T\n",
    "    df_temp.columns = df_subset.columns\n",
    "    df_subset = df_subset.append(df_temp)\n",
    "    print(f'Cluster {i}:')\n",
    "    print(df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax(), df_subset[df_subset['cluster'] == -2][df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()][0])\n",
    "    df_subset = df_subset.drop(columns=[df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()])\n",
    "    print(df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax(), df_subset[df_subset['cluster'] == -2][df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()][0])\n",
    "    df_subset = df_subset.drop(columns=[df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()])\n",
    "    print(df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax(), df_subset[df_subset['cluster'] == -2][df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()][0])\n",
    "    df_subset = df_subset.drop(columns=[df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()])\n",
    "    print(df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax(), df_subset[df_subset['cluster'] == -2][df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()][0])\n",
    "    df_subset = df_subset.drop(columns=[df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()])\n",
    "    print(df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax(), df_subset[df_subset['cluster'] == -2][df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()][0])\n",
    "    df_subset = df_subset.drop(columns=[df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()])\n",
    "    print(df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax(), df_subset[df_subset['cluster'] == -2][df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()][0])\n",
    "    df_subset = df_subset.drop(columns=[df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()])\n",
    "    print(df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax(), df_subset[df_subset['cluster'] == -2][df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()][0])\n",
    "    df_subset = df_subset.drop(columns=[df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()])\n",
    "    print(df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax(), df_subset[df_subset['cluster'] == -2][df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()][0])\n",
    "    df_subset = df_subset.drop(columns=[df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()])\n",
    "    print(df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax(), df_subset[df_subset['cluster'] == -2][df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()][0])\n",
    "    df_subset = df_subset.drop(columns=[df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()])\n",
    "    print(df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax(), df_subset[df_subset['cluster'] == -2][df_subset[df_subset['cluster'] == -1].drop(columns=df_subset.columns[-3:]).iloc[0].T.idxmax()][0])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_2017_county_data_no_pr['cluster'] = df_hc['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_2017_county_data_no_pr[full_2017_county_data_no_pr['cluster']==4].sort_values(by=['TotalPop'])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_2017_county_data_no_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_gmm = {}\n",
    "colors_gmm[0] = apply_gmm(full_normalized_2017_dataset[full_normalized_2017_dataset.columns[:-2]], seed=0, x=full_normalized_2017_dataset['Long'], y=full_normalized_2017_dataset['Lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_pca_tsne(full_normalized_2017_dataset, seed=0, colors=colors_gmm[0][3], perp_values = [5, 30, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_2017_county_data[full_2017_county_data['State Code'] != 'PR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_2017_county_data_no_pr = full_2017_county_data.copy()\n",
    "\n",
    "full_2017_county_data_no_pr = full_2017_county_data_no_pr[full_2017_county_data_no_pr['State Code'] != 'PR']\n",
    "full_2017_county_data_no_pr = full_2017_county_data_no_pr[full_2017_county_data_no_pr['State Code'] != 'HI']\n",
    "full_2017_county_data_no_pr = full_2017_county_data_no_pr[full_2017_county_data_no_pr['State Code'] != 'AK']\n",
    "\n",
    "full_unsupervised_2017_no_pr = full_2017_county_data_no_pr.iloc[:, 6:]\n",
    "\n",
    "full_unsupervised_2017_no_pr['Men'] = full_unsupervised_2017_no_pr['Men']/full_unsupervised_2017_no_pr['TotalPop']\n",
    "full_unsupervised_2017_no_pr['Women'] = full_unsupervised_2017_no_pr['Women']/full_unsupervised_2017_no_pr['TotalPop']\n",
    "full_unsupervised_2017_no_pr['VotingAgeCitizen'] = full_unsupervised_2017_no_pr['VotingAgeCitizen']/full_unsupervised_2017_no_pr['TotalPop']\n",
    "\n",
    "full_normalized_2017_dataset_no_pr = (full_unsupervised_2017_no_pr-full_unsupervised_2017_no_pr.min())/(full_unsupervised_2017_no_pr.max()-full_unsupervised_2017_no_pr.min())\n",
    "\n",
    "full_normalized_2017_dataset_no_pr = full_normalized_2017_dataset_no_pr.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_normalized_2017_dataset_no_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# NO PR\n",
    "colors_kmeans = {}\n",
    "colors_kmeans[0] = apply_kmeans(full_normalized_2017_dataset_no_pr[full_normalized_2017_dataset_no_pr.columns[:-2]], seed=0, x=full_normalized_2017_dataset_no_pr['Long'], y=full_normalized_2017_dataset_no_pr['Lat'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_pca_tsne(full_normalized_2017_dataset_no_pr[full_normalized_2017_dataset_no_pr.columns[:-2]], seed=0, colors=colors_kmeans[0][4], perp_values = [5, 30, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans = full_normalized_2017_dataset_no_pr.copy()\n",
    "df_kmeans['cluster'] = colors_kmeans[0][6]\n",
    "\n",
    "df_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne1 = TSNE(n_components=2, perplexity=30, learning_rate='auto', init='pca').fit_transform(full_normalized_2017_dataset_no_pr[full_normalized_2017_dataset_no_pr.columns[:-2]].values)\n",
    "\n",
    "colors = ['blue', 'red', 'purple', 'yellow', 'orange', 'green']\n",
    "\n",
    "f = plt.figure(figsize=(20, 8))\n",
    "gs = gridspec.GridSpec(1,3)\n",
    "\n",
    "ax1 = plt.subplot(gs[0,0:2])\n",
    "sc = ax1.scatter(df_kmeans['Long'], df_kmeans['Lat'], c=[colors[i] for i in df_kmeans['cluster']], s=4)\n",
    "ax1.legend(handles=sc.legend_elements()[0], labels=[0,1,2,3,4,5])\n",
    "\n",
    "ax1.set_xlabel('Normalized Long')\n",
    "ax1.set_ylabel('Normalized Lat')\n",
    "\n",
    "ax2 = plt.subplot(gs[0,2])\n",
    "ax2.scatter([i[0] for i in X_tsne1], [i[1] for i in X_tsne1], c=[colors[i] for i in df_kmeans['cluster']], s=4)\n",
    "\n",
    "ax1.set_title('K-Means Clustering')\n",
    "ax2.set_title('K-Means Clustering (t-SNE Visualization)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne1 = TSNE(n_components=2, perplexity=30, learning_rate='auto', init='pca').fit_transform(full_normalized_2017_dataset_no_pr[full_normalized_2017_dataset_no_pr.columns[:-2]].values)\n",
    "\n",
    "colors = ['blue', 'red', 'purple', 'yellow', 'cyan', 'green', 'orange', 'brown', 'black']\n",
    "\n",
    "f = plt.figure(figsize=(20, 8))\n",
    "gs = gridspec.GridSpec(1,3)\n",
    "\n",
    "ax1 = plt.subplot(gs[0,0:2])\n",
    "sc = ax1.scatter(df_kmeans['Long'], df_kmeans['Lat'], c=[colors[i] for i in df_kmeans['cluster']], s=5)\n",
    "ax1.legend(handles=sc.legend_elements()[0], labels=[0,1,2,3,4,5])\n",
    "\n",
    "ax1.set_xlabel('Normalized Long')\n",
    "ax1.set_ylabel('Normalized Lat')\n",
    "\n",
    "ax2 = plt.subplot(gs[0,2])\n",
    "ax2.scatter([i[0] for i in X_tsne1], [i[1] for i in X_tsne1], c=[colors[i] for i in df_kmeans['cluster']], s=4)\n",
    "\n",
    "ax1.set_title('K-Means Clustering')\n",
    "ax2.set_title('K-Means Clustering (t-SNE Visualization)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne1 = TSNE(n_components=2, perplexity=30, learning_rate='auto', init='pca').fit_transform(full_normalized_2017_dataset_no_pr[full_normalized_2017_dataset_no_pr.columns[:-2]].values)\n",
    "\n",
    "colors = ['blue', 'red', 'purple', 'yellow', 'cyan', 'green', 'orange', 'brown']\n",
    "\n",
    "f = plt.figure(figsize=(20, 8))\n",
    "gs = gridspec.GridSpec(1,3)\n",
    "\n",
    "ax1 = plt.subplot(gs[0,0:2])\n",
    "sc = ax1.scatter(df_kmeans['Long'], df_kmeans['Lat'], c=[colors[i] for i in df_kmeans['cluster']], s=6)\n",
    "ax1.legend(handles=sc.legend_elements()[0], labels=[0,1,2,3,4,5])\n",
    "\n",
    "ax1.set_xlabel('Normalized Long')\n",
    "ax1.set_ylabel('Normalized Lat')\n",
    "\n",
    "ax2 = plt.subplot(gs[0,2])\n",
    "ax2.scatter([i[0] for i in X_tsne1], [i[1] for i in X_tsne1], c=[colors[i] for i in df_kmeans['cluster']], s=4)\n",
    "\n",
    "ax1.set_title('K-Means Clustering')\n",
    "ax2.set_title('K-Means Clustering (t-SNE Visualization)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
